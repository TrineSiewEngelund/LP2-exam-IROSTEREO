{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling Irony and Stereotype Spreaders on Twitter\n",
    "### Language Processing 2\n",
    "##### Caroline Amalie Ørum-Hansen, Maja Mittag & Trine K. M. S. Engelund\n",
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import functions and libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our custom functions\n",
    "from read_files import *\n",
    "from feature_tranformers import *\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import data**\n",
    "\n",
    "We import the tweets (X) and the true labels (y), and replace the values in y with dummy values.\n",
    "\n",
    "0 = not ironic, 1 = ironic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 200) (420,)\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "X, y = get_data()\n",
    "y = np.where(y == 'NI', 0, 1) # place with dummy values\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split data**\n",
    "\n",
    "We split data in 80% train and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (336, 200) Test shape:  (84, 200)\n"
     ]
    }
   ],
   "source": [
    "# split dataset in train and test\n",
    "X_train, x_test, Y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"Train shape: \", X_train.shape, \"Test shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Features**\n",
    "\n",
    "**Author level features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=-1,\n",
       "             transformer_list=[('tfidf',\n",
       "                                TfidfVectorizer(max_df=0.9, min_df=0.01,\n",
       "                                                ngram_range=(1, 3))),\n",
       "                               ('TTR', TTR()), ('average_word', avg_word()),\n",
       "                               ('average_char', avg_char()),\n",
       "                               ('spongebob', spongebob())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FeatureUnion of features at author level\n",
    "author_features = [\n",
    "  ('tfidf', TfidfVectorizer(analyzer='word', # char n-grams\n",
    "                            ngram_range=(1,3), # use uni-, bi and trigrams\n",
    "                            max_df = 0.90, # ignore terms that appear in more than 90% of the documents\n",
    "                            min_df=0.01,)), # ignore terms that appear in less than 1% of the documents\n",
    "  ('TTR', TTR()), # type-token ratio / lexical diversity\n",
    "  ('average_word', avg_word()), # average word length\n",
    "  ('average_char', avg_char()), # average char length\n",
    "  ('spongebob', spongebob()) # Mocking Spongebob\n",
    "]\n",
    "\n",
    "author_features_combined = FeatureUnion(transformer_list=author_features, n_jobs=-1)\n",
    "author_features_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for features at author level\n",
    "author_pipe = Pipeline([\n",
    "    ('preprocesser_author', preprocess()), # preprocess the tweets\n",
    "    ('features', author_features_combined) # compute features\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweet level features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=-1,\n",
       "             transformer_list=[('emoji_sentiment_diff', emoji_sentiment_diff()),\n",
       "                               ('sentiment_incongruity',\n",
       "                                sentiment_incongruity())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FeatureUnion of features at tweet level\n",
    "tweet_features = [\n",
    "  ('emoji_sentiment_diff', emoji_sentiment_diff()),\n",
    "  ('sentiment_incongruity', sentiment_incongruity())\n",
    "]\n",
    "\n",
    "tweet_features_combined = FeatureUnion(transformer_list=tweet_features, n_jobs=-1)\n",
    "tweet_features_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for features at tweet level\n",
    "tweet_pipe = Pipeline([\n",
    "    ('preprocesser_tweet', empty2dot()), # preprocess the tweets\n",
    "    ('features', tweet_features_combined), # compute features\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=-1,\n",
       "             transformer_list=[('authors_features',\n",
       "                                Pipeline(steps=[('preprocesser_author',\n",
       "                                                 preprocess()),\n",
       "                                                ('features',\n",
       "                                                 FeatureUnion(n_jobs=-1,\n",
       "                                                              transformer_list=[('tfidf',\n",
       "                                                                                 TfidfVectorizer(max_df=0.9,\n",
       "                                                                                                 min_df=0.01,\n",
       "                                                                                                 ngram_range=(1,\n",
       "                                                                                                              3))),\n",
       "                                                                                ('TTR',\n",
       "                                                                                 TTR()),\n",
       "                                                                                ('average_word',\n",
       "                                                                                 avg_word()),\n",
       "                                                                                ('average_char',\n",
       "                                                                                 avg_char()),\n",
       "                                                                                ('spongebob',\n",
       "                                                                                 spongebob())]))])),\n",
       "                               ('tweet_features',\n",
       "                                Pipeline(steps=[('preprocesser_tweet',\n",
       "                                                 empty2dot()),\n",
       "                                                ('features',\n",
       "                                                 FeatureUnion(n_jobs=-1,\n",
       "                                                              transformer_list=[('emoji_sentiment_diff',\n",
       "                                                                                 emoji_sentiment_diff()),\n",
       "                                                                                ('sentiment_incongruity',\n",
       "                                                                                 sentiment_incongruity())]))])),\n",
       "                               ('stylometric_counts', stylometric_counts())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all features\n",
    "all_features = FeatureUnion(\n",
    "    [\n",
    "    ('authors_features', author_pipe), # features at author level\n",
    "    ('tweet_features', tweet_pipe), # features at tweet level\n",
    "    ('stylometric_counts', stylometric_counts()) # stylistic counts (also at author level)\n",
    "    ],\n",
    "    n_jobs=-1)\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pipeline**\n",
    "\n",
    "**Initialize pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(n_jobs=-1,\n",
       "                              transformer_list=[('authors_features',\n",
       "                                                 Pipeline(steps=[('preprocesser_author',\n",
       "                                                                  preprocess()),\n",
       "                                                                 ('features',\n",
       "                                                                  FeatureUnion(n_jobs=-1,\n",
       "                                                                               transformer_list=[('tfidf',\n",
       "                                                                                                  TfidfVectorizer(max_df=0.9,\n",
       "                                                                                                                  min_df=0.01,\n",
       "                                                                                                                  ngram_range=(1,\n",
       "                                                                                                                               3))),\n",
       "                                                                                                 ('TTR',\n",
       "                                                                                                  TTR()),\n",
       "                                                                                                 ('average_word',\n",
       "                                                                                                  avg_word()),\n",
       "                                                                                                 ('average_char',\n",
       "                                                                                                  avg_char()),\n",
       "                                                                                                 ('spongebob',\n",
       "                                                                                                  spongebob())]))])),\n",
       "                                                ('tweet_features',\n",
       "                                                 Pipeline(steps=[('preprocesser_tweet',\n",
       "                                                                  empty2dot()),\n",
       "                                                                 ('features',\n",
       "                                                                  FeatureUnion(n_jobs=-1,\n",
       "                                                                               transformer_list=[('emoji_sentiment_diff',\n",
       "                                                                                                  emoji_sentiment_diff()),\n",
       "                                                                                                 ('sentiment_incongruity',\n",
       "                                                                                                  sentiment_incongruity())]))])),\n",
       "                                                ('stylometric_counts',\n",
       "                                                 stylometric_counts())])),\n",
       "                ('scaler', MaxAbsScaler()), ('classifier', SVC())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate pipeline \n",
    "pipe = Pipeline([\n",
    "    ('features', all_features), # compute features\n",
    "    ('scaler', MaxAbsScaler()), # scale features\n",
    "    ('classifier', SVC()), # run classifier (SVC is just a placeholder)\n",
    "])\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gridsearch**\n",
    "\n",
    "**Parameter grid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid for classifiers\n",
    "param_grid = [\n",
    "    # SVM\n",
    "    {\n",
    "        'classifier': [SVC()],\n",
    "        'classifier__kernel': ['linear'],\n",
    "        'classifier__gamma': [0.1, 5],\n",
    "        'classifier__C': [0.001, 1000]\n",
    "    },\n",
    "\n",
    "    # Random Forrest\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier()],\n",
    "        'classifier__n_estimators': [200, 300],\n",
    "        'classifier__n_jobs': [-1]\n",
    "    },\n",
    "\n",
    "    # Logistic Regression\n",
    "    {\n",
    "        'classifier': [LogisticRegression()],\n",
    "        'classifier__solver': ['liblinear'],\n",
    "        'classifier__C': [0.01, 1, 100]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-fold gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=-1,\n",
       "                                                     transformer_list=[('authors_features',\n",
       "                                                                        Pipeline(steps=[('preprocesser_author',\n",
       "                                                                                         preprocess()),\n",
       "                                                                                        ('features',\n",
       "                                                                                         FeatureUnion(n_jobs=-1,\n",
       "                                                                                                      transformer_list=[('tfidf',\n",
       "                                                                                                                         TfidfVectorizer(max_df=0.9,\n",
       "                                                                                                                                         min_df=0.01,\n",
       "                                                                                                                                         ngram_range=(1,\n",
       "                                                                                                                                                      3))),\n",
       "                                                                                                                        ('TTR',\n",
       "                                                                                                                         TTR()),\n",
       "                                                                                                                        ('average_word',\n",
       "                                                                                                                         avg_word()),\n",
       "                                                                                                                        ('average_char',\n",
       "                                                                                                                         a...\n",
       "                          'classifier__gamma': [0.1, 5],\n",
       "                          'classifier__kernel': ['linear']},\n",
       "                         {'classifier': [RandomForestClassifier(n_estimators=300,\n",
       "                                                                n_jobs=-1)],\n",
       "                          'classifier__n_estimators': [200, 300],\n",
       "                          'classifier__n_jobs': [-1]},\n",
       "                         {'classifier': [LogisticRegression()],\n",
       "                          'classifier__C': [0.01, 1, 100],\n",
       "                          'classifier__solver': ['liblinear']}],\n",
       "             refit='accuracy', return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gridsearch\n",
    "grid_search = GridSearchCV( pipe,\n",
    "                            param_grid=param_grid,\n",
    "                            cv=5,\n",
    "                            scoring='accuracy', \n",
    "                            refit='accuracy', \n",
    "                            n_jobs=-1, \n",
    "                            return_train_score=True)\n",
    "grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "\n",
    "**Best parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(n_jobs=-1,\n",
       "                              transformer_list=[('authors_features',\n",
       "                                                 Pipeline(steps=[('preprocesser_author',\n",
       "                                                                  preprocess()),\n",
       "                                                                 ('features',\n",
       "                                                                  FeatureUnion(n_jobs=-1,\n",
       "                                                                               transformer_list=[('tfidf',\n",
       "                                                                                                  TfidfVectorizer(max_df=0.9,\n",
       "                                                                                                                  min_df=0.01,\n",
       "                                                                                                                  ngram_range=(1,\n",
       "                                                                                                                               3))),\n",
       "                                                                                                 ('TTR',\n",
       "                                                                                                  TTR()),\n",
       "                                                                                                 ('average_word',\n",
       "                                                                                                  avg_word()),\n",
       "                                                                                                 ('average_char',\n",
       "                                                                                                  avg_char()),\n",
       "                                                                                                 ('spongebob',\n",
       "                                                                                                  spon...\n",
       "                                                ('tweet_features',\n",
       "                                                 Pipeline(steps=[('preprocesser_tweet',\n",
       "                                                                  empty2dot()),\n",
       "                                                                 ('features',\n",
       "                                                                  FeatureUnion(n_jobs=-1,\n",
       "                                                                               transformer_list=[('emoji_sentiment_diff',\n",
       "                                                                                                  emoji_sentiment_diff()),\n",
       "                                                                                                 ('sentiment_incongruity',\n",
       "                                                                                                  sentiment_incongruity())]))])),\n",
       "                                                ('stylometric_counts',\n",
       "                                                 stylometric_counts())])),\n",
       "                ('scaler', MaxAbsScaler()),\n",
       "                ('classifier',\n",
       "                 RandomForestClassifier(n_estimators=300, n_jobs=-1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print best parameters\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean cross-validation accuracy of best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8808604038630378"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and test accuracy of best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", grid_search.score(X_train,Y_train))\n",
    "print(\"Test accuracy:\", grid_search.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results from each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time',\n",
      "       'param_classifier', 'param_classifier__C', 'param_classifier__gamma',\n",
      "       'param_classifier__kernel', 'param_classifier__n_estimators',\n",
      "       'param_classifier__n_jobs', 'param_classifier__solver', 'params',\n",
      "       'split0_test_score', 'split1_test_score', 'split2_test_score',\n",
      "       'split3_test_score', 'split4_test_score', 'mean_test_score',\n",
      "       'std_test_score', 'rank_test_score', 'split0_train_score',\n",
      "       'split1_train_score', 'split2_train_score', 'split3_train_score',\n",
      "       'split4_train_score', 'mean_train_score', 'std_train_score'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>param_classifier__gamma</th>\n",
       "      <th>param_classifier__kernel</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>param_classifier__n_jobs</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236.719534</td>\n",
       "      <td>2.906942</td>\n",
       "      <td>52.474745</td>\n",
       "      <td>1.731977</td>\n",
       "      <td>SVC()</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868964</td>\n",
       "      <td>0.024232</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.718771</td>\n",
       "      <td>13.415928</td>\n",
       "      <td>54.650036</td>\n",
       "      <td>6.829754</td>\n",
       "      <td>SVC()</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868964</td>\n",
       "      <td>0.024232</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220.953818</td>\n",
       "      <td>4.139174</td>\n",
       "      <td>54.228982</td>\n",
       "      <td>2.005914</td>\n",
       "      <td>SVC()</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877875</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215.750901</td>\n",
       "      <td>21.997525</td>\n",
       "      <td>51.319964</td>\n",
       "      <td>1.881687</td>\n",
       "      <td>SVC()</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877875</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221.354735</td>\n",
       "      <td>10.698988</td>\n",
       "      <td>53.100247</td>\n",
       "      <td>4.504647</td>\n",
       "      <td>RandomForestClassifier(n_estimators=300, n_job...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863038</td>\n",
       "      <td>0.034743</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>218.500857</td>\n",
       "      <td>10.791893</td>\n",
       "      <td>51.896949</td>\n",
       "      <td>2.008467</td>\n",
       "      <td>RandomForestClassifier(n_estimators=300, n_job...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880860</td>\n",
       "      <td>0.039126</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>211.294241</td>\n",
       "      <td>15.759943</td>\n",
       "      <td>52.588498</td>\n",
       "      <td>3.567639</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857112</td>\n",
       "      <td>0.026171</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>218.928298</td>\n",
       "      <td>11.383826</td>\n",
       "      <td>53.679711</td>\n",
       "      <td>2.642386</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860053</td>\n",
       "      <td>0.027955</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200.300083</td>\n",
       "      <td>5.940365</td>\n",
       "      <td>43.077080</td>\n",
       "      <td>2.829122</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866023</td>\n",
       "      <td>0.031483</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     236.719534      2.906942        52.474745        1.731977   \n",
       "1     230.718771     13.415928        54.650036        6.829754   \n",
       "2     220.953818      4.139174        54.228982        2.005914   \n",
       "3     215.750901     21.997525        51.319964        1.881687   \n",
       "4     221.354735     10.698988        53.100247        4.504647   \n",
       "5     218.500857     10.791893        51.896949        2.008467   \n",
       "6     211.294241     15.759943        52.588498        3.567639   \n",
       "7     218.928298     11.383826        53.679711        2.642386   \n",
       "8     200.300083      5.940365        43.077080        2.829122   \n",
       "\n",
       "                                    param_classifier param_classifier__C  \\\n",
       "0                                              SVC()               0.001   \n",
       "1                                              SVC()               0.001   \n",
       "2                                              SVC()                1000   \n",
       "3                                              SVC()                1000   \n",
       "4  RandomForestClassifier(n_estimators=300, n_job...                 NaN   \n",
       "5  RandomForestClassifier(n_estimators=300, n_job...                 NaN   \n",
       "6                               LogisticRegression()                0.01   \n",
       "7                               LogisticRegression()                   1   \n",
       "8                               LogisticRegression()                 100   \n",
       "\n",
       "  param_classifier__gamma param_classifier__kernel  \\\n",
       "0                     0.1                   linear   \n",
       "1                       5                   linear   \n",
       "2                     0.1                   linear   \n",
       "3                       5                   linear   \n",
       "4                     NaN                      NaN   \n",
       "5                     NaN                      NaN   \n",
       "6                     NaN                      NaN   \n",
       "7                     NaN                      NaN   \n",
       "8                     NaN                      NaN   \n",
       "\n",
       "  param_classifier__n_estimators param_classifier__n_jobs  ...  \\\n",
       "0                            NaN                      NaN  ...   \n",
       "1                            NaN                      NaN  ...   \n",
       "2                            NaN                      NaN  ...   \n",
       "3                            NaN                      NaN  ...   \n",
       "4                            200                       -1  ...   \n",
       "5                            300                       -1  ...   \n",
       "6                            NaN                      NaN  ...   \n",
       "7                            NaN                      NaN  ...   \n",
       "8                            NaN                      NaN  ...   \n",
       "\n",
       "  mean_test_score std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        0.868964       0.024232                4                 1.0   \n",
       "1        0.868964       0.024232                4                 1.0   \n",
       "2        0.877875       0.029243                2                 1.0   \n",
       "3        0.877875       0.029243                2                 1.0   \n",
       "4        0.863038       0.034743                7                 1.0   \n",
       "5        0.880860       0.039126                1                 1.0   \n",
       "6        0.857112       0.026171                9                 1.0   \n",
       "7        0.860053       0.027955                8                 1.0   \n",
       "8        0.866023       0.031483                6                 1.0   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0                 1.0                 1.0                 1.0   \n",
       "1                 1.0                 1.0                 1.0   \n",
       "2                 1.0                 1.0                 1.0   \n",
       "3                 1.0                 1.0                 1.0   \n",
       "4                 1.0                 1.0                 1.0   \n",
       "5                 1.0                 1.0                 1.0   \n",
       "6                 1.0                 1.0                 1.0   \n",
       "7                 1.0                 1.0                 1.0   \n",
       "8                 1.0                 1.0                 1.0   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0                 1.0               1.0              0.0  \n",
       "1                 1.0               1.0              0.0  \n",
       "2                 1.0               1.0              0.0  \n",
       "3                 1.0               1.0              0.0  \n",
       "4                 1.0               1.0              0.0  \n",
       "5                 1.0               1.0              0.0  \n",
       "6                 1.0               1.0              0.0  \n",
       "7                 1.0               1.0              0.0  \n",
       "8                 1.0               1.0              0.0  \n",
       "\n",
       "[9 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save gridsearch results in dataframe\n",
    "scoring_results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(scoring_results.columns)\n",
    "scoring_results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
